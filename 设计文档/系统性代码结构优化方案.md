# 系统性代码结构优化方案（AI 面试工作流）

## 目标
- 降低理解成本：将回调、状态、配置、资源管理分离，提升可读性。
- 提升可维护性：引入强类型配置与清晰的职责边界，减小修改的影响面。
- 保持行为一致：在重构早期严格不改变业务逻辑与外部接口表现。

## 现状问题简述
- 回调参数膨胀：`DefaultInterviewAgent.start(...)` 内联匿名回调较多，阅读负担重。
- 配置散落：`AgentFactory` 从系统属性与 `SessionContext` 混合读取，弱类型难以追踪。
- 状态与策略混杂：STT/LLM 事件处理、拼接答案、持久化逻辑散落在服务类中。
- 资源管理分散：线程池与连接的创建/销毁分布在多处，生命周期不易统一把控。

## 重构原则
- 行为等价：除日志与命名外，外部行为不变化，按阶段推进。
- 小步快跑：每一步修改都能独立编译通过、易于回滚。
- 明确边界：职责分离（回调聚合、配置对象、持久化服务、资源管理）。
- 保持风格：沿用现有模块与包结构，减少无关变动。

## 分阶段计划
1) 最小改动、聚合回调与持久化
- 引入 `AgentCallbacks`（或回调聚合类）以命名方式管理事件处理，减少匿名函数。
- 引入 `MessagePersistenceService` 统一消息持久化（线程池、异常处理、Mapper 调用）。
- 引入 `AnswerAccumulator` 代替 `StringBuilder` 的同步片段，简化拼接与清空操作。

2) 强类型配置与工厂解耦
- 新增 `AgentConfig`（STT/LLM/策略等），替代分散的系统属性读取。
- 优化 `AgentFactory` 使其只接收 `AgentConfig` 并返回具体客户端，移除隐式全局状态。

3) 责任分离与资源管理统一
- 将 `DefaultInterviewAgent` 聚焦于编排（事件流转与生命周期），将策略逻辑下沉到独立类。
- 统一线程池与连接的创建/销毁到专用的资源管理器或 Spring 管理的 Bean。

4) 端点与服务解耦
- WebSocket Handler 与服务层通过明确接口交互，移除静态注入方式。
- 保留日志分类与追踪字段（会话 ID、WS 会话 ID）以便定位问题。

## 目录与包结构建议（在现有结构上增量）
- `interview-infra`：
  - `domain/session/AudioStreamServiceImpl`（保持）
  - `domain/session/MessagePersistenceService`（新增）
  - `domain/session/util/AnswerAccumulator`（新增）
- `interview-domain`：
  - `domain/agent/DefaultInterviewAgent`（后续拆分策略类）
  - `domain/agent/AgentConfig` / `AgentCallbacks`（后续新增）
- `interview-infra/repo/...`：Mapper 与 XML 按现状保持

## 接口与类设计草图（核心增量）
- `MessagePersistenceService`
  - 方法：`persistUser(sessionId, content)`、`persistAssistant(sessionId, content)`
  - 特性：内部线程池异步持久化，统一异常处理与日志记录，生命周期托管

- `AnswerAccumulator`
  - 方法：`append(delta)`、`drain()`
  - 特性：线程安全累加与一次性清空，避免外层细粒度同步块

- `AgentCallbacks`（后续）
  - 字段：`onPartialText`、`onFinalText`、`onQuestion`、`onAnswerDelta`、`onAnswerComplete`、`onError`、`onSttReady`
  - 作用：命名回调集合，替代匿名 lambda 传参，提升可读性

- `AgentConfig`（后续）
  - 字段：STT/LLM 提供商、key、模型、速率、策略开关等
  - 作用：强类型配置，作为 `AgentFactory` 与 `DefaultInterviewAgent` 的唯一输入配置

## 迁移与风险控制
- 按阶段提交代码，每步编译验证（`-pl interview-infra -am compile`）。
- 引入新服务类时保留旧逻辑路径，可通过开关快速回退。
- 增强日志上下文（会话 ID、WS 会话 ID、事件类型）便于排查。

## 验证与度量
- 编译与单元测试：确保关键路径（`AudioStreamServiceImpl`、`DefaultInterviewAgent`）可编译。
- 运行时验证：本地启动后进行一轮问答验证事件序列与消息落库。
- 质量度量：函数长度、匿名函数数量、共享状态点数量减少；命名一致性提升。

## 时间计划（建议）
- 第 1 步（当日）：新增持久化服务与累加器，改造 `AudioStreamServiceImpl` 调用，编译验证。
- 第 2 步（+1 天）：引入 `AgentConfig` 与 `AgentCallbacks`，改造 `DefaultInterviewAgent` 与 `AgentFactory`。
- 第 3 步（+1 天）：资源管理统一与策略类下沉，清理残留耦合。

## 实施步骤 1（当前要做）
- 新增 `MessagePersistenceService` 与 `AnswerAccumulator`。
- 改造 `AudioStreamServiceImpl`：
  - 用 `MessagePersistenceService` 替代直接调用 Mapper 的异步持久化逻辑。
  - 用 `AnswerAccumulator` 替代 `StringBuilder`+同步块的答案拼接与清空。
- 编译 `interview-infra` 模块并验证行为未变（日志与落库）。

## 相似问题匹配（LLM 驱动）优化方案

### 目标
- 不再使用单独的相似度算法（Jaccard/向量等），改为用大模型裁决“是否为新问题”。
- 明确业务规则：如果检测到的是差不多的问题则不输出新问题（合并到当前问题）。
- 通过 Prompt 设计让模型进行“同题/补充/新问题/无问题”的判定，并在需要时返回规范化问题。

### 行为规则
- 仅当大模型判定为 `NEW` 时触发新问题与新一轮回答；否则不触发新问题。
- 判定为 `SAME` 或 `ELABORATION`（补充）时：
  - 不触发新问题事件（不重启回答流）。
  - 可选择更新当前问题的规范化文本或上下文（合并补充信息）。
- 判定为 `NONE` 时视为无问题输入（例如闲聊或说明性文本）。

### Prompt 设计（示例）
- System Prompt：
  - 你是面试问题去重器与规范化器。任务：根据“最近问题”“当前输入”“上下文”判断是否为同题或只是补充，不要产生新问题。仅在确认为“新问题”时返回 NEW，否则返回 SAME 或 ELABORATION，并给出规范化后的问题（如需合并补充）。遵循：如果检测到的是差不多的问题则不输出新问题。中文输出，避免虚构。
- User Prompt：
  - 最近问题：[lastQuestion]
  - 当前输入：[currentText]
  - 上下文：[contextString]
  - 请判断类别并返回 JSON：
    {"class": "SAME|ELABORATION|NEW|NONE", "canonical": "...", "reason": "..."}
- Few-shot 示例：
  - 示例 A：last="Redis 的应用场景是什么？"；current="结合实习经历说一下 Redis 的应用场景。" → class=ELABORATION，canonical="Redis 的应用场景（结合实习经历）"；不触发新问题。
  - 示例 B：last="Go 的协程与线程关系？"；current="它的线程模型是什么？"（上下文指代 Go）→ class=SAME 或 ELABORATION；不触发新问题。
  - 示例 C：last="Kafka 消息有序如何保证？"；current="Kafka 消息乱序如何处理？" → class=NEW；触发新问题。

### 接入流程（代码层面）
- `DefaultInterviewAgent`：
  - 在 `extractQuestion` 之后，调用 `llmClient.judgeQuestionEquivalence(lastQuestion, candidate, context)`。
  - 根据返回的 `class`：
    - `NEW`：调用 `generateAnswerStream`/`generateAnswer` 开启新回答，并将 `candidate` 作为新 `lastQuestion`（可使用返回的 `canonical`）。
    - `SAME`/`ELABORATION`：不触发新问题；可调用轻量逻辑合并 `canonical` 到当前问题文本，或仅更新上下文；避免重启回答流。
    - `NONE`：忽略。
  - 取消或弱化现有 `jaccardSimilarity` 门控，当 `useLlmSimilarity=true` 时以 LLM 判定为准。
- `AliyunLlmClient`：
  - 新增 `judgeQuestionEquivalence(lastQuestion, candidate, context)` 方法，使用 DashScope 文本生成，按上述 Prompt 返回结构化结果。
- `ConversationContextBuilder`：
  - 支持将 `ELABORATION` 的补充信息合并到当前问题的上下文字符串中（不改变问题触发节奏）。

### 配置与开关
- `AgentConfig.llmSimilarity.enabled`（默认开启）：启用 LLM 判定，停用相似度算法门控。
- `AgentConfig.llmSimilarity.promptVersion`：便于灰度与 A/B 测试不同 Prompt。
- `AgentConfig.llmSimilarity.timeoutMillis`：控制判定调用的超时与失败策略。
- `AgentConfig.answerOnlyOnQuestion`：沿用现有开关；当 `ELABORATION` 时不启动新回答（保持稳定）。
- 兼容性：当 `enabled=false` 时，回退到现有逻辑（可选择保留/移除 Jaccard 门控）。

### 降级与容错
- LLM 超时/异常时默认判定为 `SAME`（保守不触发新问题，避免回答抖动）。
- 可选（默认关闭）：在异常时启用字符 n-gram 简易门控作为兜底，避免误触发；该兜底不作为常态算法，只用于容错。

### 监控与度量
- `llm_similarity_calls_total`、`llm_similarity_error_total`、`llm_similarity_latency_ms_avg`。
- `llm_similarity_class_ratio`（SAME/ELABORATION/NEW/NONE 分布）。
- `question_new_triggered_total` 与 `question_suppressed_total`（抑制成功率）。
- 配合日志：记录 `sessionId`、`wsSessionId`、分类结果与原因，便于迭代 Prompt。

### 验证计划
- 离线回放：采集典型中文问答样本，跑判定分布与抑制率。
- 联调验证：本地对话测试，观测回答流是否稳定、不重启。
- 灰度发布：按 `promptVersion` 分流，比较 NEW 触发率与用户体验。

### 实施步骤（后续）
- 新增 `AliyunLlmClient.judgeQuestionEquivalence(...)` 与配置项。
- 在 `DefaultInterviewAgent` 替换新问题门控为 LLM 判定，保留异常兜底。
- 补充 `ConversationContextBuilder` 合并补充信息的能力（可选）。
- 增加度量与日志字段，编译与联调验证。

## 对话记忆管理与 STT 触发优化方案

### 目标
- 提升多轮对话记忆的准确与稳态，减少指代与补充导致的新问题误触发。
- STT 尽量“不丢信息”，识别时机与合并策略交由 LLM 主导；仅在明确新问题时触发回答。

### 记忆管理
- 工作记忆分层：
  - 工作窗（短期）：保留最近 N 个规范化问题与最近 M 条发言摘要，用于回答上下文构建。
  - 关键事实表：抽取实体与要点（公司、岗位、项目、指标、技术栈）为键值对，避免上下文膨胀。
  - 当前问题状态：`currentQuestion.canonical` 与 `currentQuestion.accumulatedContext`，将 `ELABORATION` 的补充累积到当前问题。
- LLM 驱动更新：
  - 在 `NEW` 或 `ELABORATION` 判定后触发“滚动摘要 + 关键事实抽取”，更新 `ConversationContextBuilder` 的上下文。
  - 做指代消解与术语统一（例如“它的线程模型是什么？”→绑定至“Go 的线程模型”）。
  - 摘要粒度控制：保留 3–5 条要点；长期信息进入“关键事实表”，短期细节进入“当前问题上下文”。

### STT 触发时机
- 两段式提交：
  - 部分（partial）累积：软端点与标点提前提交，但所有 partial 进入 `partialBuffer` 保留，不丢弃。
  - 最终（final）合并：软端点或显著停顿时，将 partial 合并为 segment 进入问题识别流程。
- 触发规则：
  - 长度阈值 `minCharsForDetection`（建议 40–60 字符）；未达阈值仅累积，不调用 LLM。
  - 标点与疑问词增强：遇到 `?`、`？` 或“怎么/为何/是否/能否/有什么”时提高触发优先级。
  - 自适应节流：回答生成期间的 800–1200ms 抑制窗口，避免抖动重复触发。
- LLM 前置容错：
  - 对早触发的 partial，LLM 能判定 `SAME/ELABORATION/NONE`，避免误判为新问题。
  - 当 `extractQuestion` 返回“无问题”但存在补充信号时，调用“段落关系判定”以合并补充内容。

### LLM 协作
- 已有链路：`extractQuestion` 后调用 `judgeQuestionEquivalence(lastQuestion, candidate, context)`，仅 `NEW` 触发新问题。
- 新增段落关系判定：
  - 当 `extractQuestion` 返回“无问题”时，调用 `judgeSegmentRelation(lastQuestion, segment, context)`，仅判定 `ELABORATION/NONE`。
  - `ELABORATION`：合并到 `currentQuestion.accumulatedContext`，不触发新问题；`NONE`：忽略。
- 记忆更新接口：
  - 新增 `llmClient.updateContextMemory(currentQuestion, recentUtterances, facts)`，返回 `{summary, facts}`，由 `ConversationContextBuilder` 应用。

### 参数建议
- `softEndpointMillis`：1200–1800ms；`maxSegmentChars`：200–300；`minCharsForDetection`：40–60。
- `answerOnlyOnQuestion`：保持 `true`；`llmSimilarity.enabled`：保持 `true`。
- 自适应抑制窗口：回答期间的 800–1200ms 不触发新问题判定。

### 监控与度量
- 触发度量：`question_new_triggered_total`、`question_suppressed_total`。
- 判定分布：`llm_similarity_class_ratio`（SAME/ELABORATION/NEW/NONE）。
- 记忆质量：摘要长度、事实条目数、上下文字符串长度。
- 性能：LLM 调用延迟与异常比；STT 段长度与软端点分布。

### 落地步骤
- 扩展 `ConversationContextBuilder`：`addElaborationText`、`mergeFacts`、`getRollingSummary`。
- 扩展 `LlmClient`：新增 `judgeSegmentRelation` 与 `updateContextMemory`。
- 在 `DefaultInterviewAgent` 接入：
  - `extractQuestion=无问题` 路径调用 `judgeSegmentRelation` 合并补充。
  - 在 `NEW` 后调用 `updateContextMemory` 更新上下文（ELABORATION 低频调用）。
- 新增配置：`context.minCharsForDetection`、`context.adaptiveSuppressionMillis`；为 `updateContextMemory` 设置冷却窗口。
- 验证与灰度：本地回放与联调，按版本分流灰度上线。